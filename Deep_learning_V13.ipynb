{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests \n",
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import text,sequence\n",
    "import collections\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, LSTM,Embedding\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D ,Dropout\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Bidirectional, LSTM\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tag</th>\n",
       "      <th>patterns</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hardwarenewton</td>\n",
       "      <td>my printer capabl print document</td>\n",
       "      <td>ok you want order new toner lnk what kind color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hardwarenewton</td>\n",
       "      <td>my printer print paper</td>\n",
       "      <td>ok you want order new toner lnk what kind color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hardwarenewton</td>\n",
       "      <td>i think lnk empti</td>\n",
       "      <td>ok you want order new toner lnk what kind color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>hardwarenewton</td>\n",
       "      <td>i believ lnk empti</td>\n",
       "      <td>ok you want order new toner lnk what kind color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hardwarenewton</td>\n",
       "      <td>the lnk appear empti</td>\n",
       "      <td>ok you want order new toner lnk what kind color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>hardwarenewton</td>\n",
       "      <td>i believ lnk unfil</td>\n",
       "      <td>ok you want order new toner lnk what kind color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>hardwarenewton</td>\n",
       "      <td>my printer toner cartridg</td>\n",
       "      <td>ok you want order new toner lnk what kind color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>hardwarenewton</td>\n",
       "      <td>my printer toner</td>\n",
       "      <td>ok you want order new toner lnk what kind color</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             tag                            patterns  \\\n",
       "0           0  hardwarenewton   my printer capabl print document    \n",
       "1           1  hardwarenewton             my printer print paper    \n",
       "2           2  hardwarenewton                  i think lnk empti    \n",
       "3           3  hardwarenewton                 i believ lnk empti    \n",
       "4           4  hardwarenewton               the lnk appear empti    \n",
       "5           5  hardwarenewton                 i believ lnk unfil    \n",
       "6           6  hardwarenewton          my printer toner cartridg    \n",
       "7           7  hardwarenewton                   my printer toner    \n",
       "\n",
       "                                           responses  \n",
       "0   ok you want order new toner lnk what kind color   \n",
       "1   ok you want order new toner lnk what kind color   \n",
       "2   ok you want order new toner lnk what kind color   \n",
       "3   ok you want order new toner lnk what kind color   \n",
       "4   ok you want order new toner lnk what kind color   \n",
       "5   ok you want order new toner lnk what kind color   \n",
       "6   ok you want order new toner lnk what kind color   \n",
       "7   ok you want order new toner lnk what kind color   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df = pd.read_csv(\"Toner_updatedDisNew4.csv\")\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drob the Unnamed column \n",
    "df.drop('Unnamed: 0',\n",
    "axis='columns', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat new column\n",
    "df['Class'] = df['tag'].map({' catagorysoftwar ':0,' hardwarenewprint ':1,'hardwarenewton':2,' subcatagorysoftwar ':3,' hardwarenewprinterjustif ':4,' subjectnewemail ':5,'hardwarenewtonercolor':6, ' greet ':7, ' thank ':8, ' name ':9, ' goodby ':\"10\" , ' item ':11, ' location2 ':12,' contactinformation2 ':13, ' location3 ':14, ' contact information3 ':15, 'hardwarenewtonerisdnam':16,' contact information1 ':17, 'hardwarenewtonerprintermodel':18, ' location1 ':19})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>patterns</th>\n",
       "      <th>responses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>hardwarenewprinterjustif</td>\n",
       "      <td>i want print document printer</td>\n",
       "      <td>ok problem give inform pleas name</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>name</td>\n",
       "      <td>ahmad alharthi</td>\n",
       "      <td>ok extens number pleas</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>hardwarenewprint</td>\n",
       "      <td>it would great i new printer offic</td>\n",
       "      <td>ok sure justif order printer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>subcatagorysoftwar</td>\n",
       "      <td>a softwar applic need uninstal</td>\n",
       "      <td>ok i need contact inform pleas name</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>hardwarenewprinterjustif</td>\n",
       "      <td>i need printer sinc i one</td>\n",
       "      <td>ok problem give inform pleas name</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>catagorysoftwar</td>\n",
       "      <td>we problem softwar</td>\n",
       "      <td>what kind softwar issu have</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>hardwarenewtonercolor</td>\n",
       "      <td>i d want black lnk</td>\n",
       "      <td>your order toner record jest i want ask isd n...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>hardwarenewprinterjustif</td>\n",
       "      <td>the paper i need print need print printer</td>\n",
       "      <td>ok problem give inform pleas name</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>hardwarenewprint</td>\n",
       "      <td>a new printer need offic</td>\n",
       "      <td>ok sure justif order printer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>subjectnewemail</td>\n",
       "      <td>i m brandnew employe email address</td>\n",
       "      <td>fine what item need depart email user email em...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            tag                                     patterns  \\\n",
       "222   hardwarenewprinterjustif                i want print document printer    \n",
       "267                       name                               ahmad alharthi    \n",
       "175           hardwarenewprint           it would great i new printer offic    \n",
       "135         subcatagorysoftwar               a softwar applic need uninstal    \n",
       "201   hardwarenewprinterjustif                    i need printer sinc i one    \n",
       "96             catagorysoftwar                           we problem softwar    \n",
       "48        hardwarenewtonercolor                          i d want black lnk    \n",
       "221   hardwarenewprinterjustif    the paper i need print need print printer    \n",
       "191           hardwarenewprint                     a new printer need offic    \n",
       "245            subjectnewemail           i m brandnew employe email address    \n",
       "\n",
       "                                             responses Class  \n",
       "222                 ok problem give inform pleas name      4  \n",
       "267                            ok extens number pleas      9  \n",
       "175                      ok sure justif order printer      1  \n",
       "135               ok i need contact inform pleas name      3  \n",
       "201                 ok problem give inform pleas name      4  \n",
       "96                        what kind softwar issu have      0  \n",
       "48    your order toner record jest i want ask isd n...     6  \n",
       "221                 ok problem give inform pleas name      4  \n",
       "191                      ok sure justif order printer      1  \n",
       "245  fine what item need depart email user email em...     5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words from english library in general "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"patterns\"].values\n",
    "#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process\n",
    "X = []\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "for par in df[\"patterns\"].values:\n",
    "    tmp = []\n",
    "    sentences = nltk.sent_tokenize(par)\n",
    "    for sent in sentences:\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]\n",
    "        tmp.extend(filtered_words)\n",
    "    X.append(tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' my printer capabl print document   my printer print paper   i think lnk empti   i believ lnk empti   the lnk appear empti   i believ lnk unfil   my printer toner cartridg   my printer toner   my printer lnk   my printer lack lnk   my lnk empti   my toner empti   i want order new lnk   i d want order new lnk   i want order new toner   i want order new black lnk   i want order new black toner   i want request new toner   i d like request replac toner cartridg   i d want order new toner cartridg   i toner cartridg printer   there toner cartridg printer   i can not use printer toner cartridg empti   a toner cartridg printer   my printer lack toner cartridg   it toner cartridg   the toner cartridg printer empti   the printer toner cartridg  â€œ i want black lnk â€\\u200c  i want black toner   i would like black lnk   black lnk i need   it would nice black lnk   pleas send black lnk   it s black lnk i want   black color   i need color   i need color   all color need   i need color   i need red blue   i need red blue   i want yellow blue   i need yellow blue   i need black blue   i want black blue   i m look black lnk   i d like black lnk   i d want black lnk   i m look black toner   i need black toner   my isd name â€¦   my printer model    i problem softwar   there problem softwar   the softwar work me   i m troubl softwar   my softwar work   softwar work me   a softwar issu arisen   softwar give problem   softwar caus problem   the softwar i use work me   the softwar work   there problem softwar   softwar work   i unabl use softwar   the softwar work   unfortun softwar work   i can not use softwar   i can not run softwar   we experienc problem softwar   no softwar work   i ca nt open softwar   the softwar open   i unabl open softwar   i can not open softwar   have troubl open softwar   open softwar i ca nt   open softwar open   i unabl access softwar   my softwar open   this softwar open   i issu softwar   i problem softwar   the softwar work me   a softwar issu arisen   i troubl softwar   the softwar work proper me   i issu softwar   there s problem softwar   this softwar give problem   the softwar broken   there problem softwar   softwar malfunct   it work   we problem softwar   softwar broken   i troubl softwar   there problem softwar   my softwar work   the softwar give problem   the softwar i m use work me   i m problem softwar   there issu softwar   a problem arisen softwar   have troubl softwar   the softwar i use caus troubl   the softwar caus problem me   i m problem softwar   i m experienc issu softwar   the program work   the applic work   softwar problem   i want instal softwar applic   i want instal piec softwar   instal softwar program   a softwar applic need instal   it s time instal softwar   there softwar applic i wish instal   the softwar applic i would like instal   my comput need instal softwar applic   it s time instal program   instal softwar program   softwar program need instal   i want uninstal softwar applic   uninstal softwar applic i want do   uninstal program i need do   i want uninstal piec softwar   a softwar program need uninstal   uninstal softwar goal   remov softwar applic comput   there softwar program i wish uninstal   how i uninstal program   the softwar applic i want remov need uninstal   a softwar applic need uninstal   an evalu softwar applic   evalu softwar applic   do need medic applic servic   do need mobil applic servic   do need non medic servic   do need non medic servic   i need new printer   i printer offic   i want printer   need new printer   could pleas give printer   can i printer   can i printer offic   pleas get new printer   pleas suppli new printer   pleas get new printer   in offic i printer   in offic i m without printer   my offic printer   in offic i m desper need new printer   in offic i requir new printer   in offic i m need new printer   is possibl printer offic   is possibl printer workplac   is possibl printer offic   pleas provid new printer offic   pleas provid offic new printer   i need printer pleas   pleas give printer   it would help could give printer   i would like new printer offic   i need new printer offic   my offic need new printer pleas   i need new printer offic   pleas provid new printer offic   would pleas provid new printer offic   in order improv product offic i want new printer   in offic i would like get new printer   it would nice new printer offic   it would great i new printer offic   a new printer help offic   is possibl printer offic   would possibl printer offic   there printer offic i one   get new printer offic   i d like new printer offic   i d appreci new printer offic   let get new printer offic   i would realli appreci could get new laser printer offic   i need new offic printer   let new printer offic   my offic need new printer could pleas provid one   is way i get new printer offic   i would appreci receiv new printer offic   i would like obtain new printer offic   a new printer need offic   in offic i d like new printer   in offic let new printer   a new offic printer need   an offic printer need   i dont one   i printer i need one   sinc i printer i need one   have printer problem me   my printer broken i need get one   i need printer sinc i one   the reason i printer i need one   i need printer i one   have printer necess me   it imper i printer   in order fulfil need i must printer   the printer i broken i need new one   there problem printer i need one   a printer need mine broken   i need printer sinc mine broken   i m need printer current one broke   the printer offic broken i need anoth one   my printer work i need new one   my printer work i want new one   i need printer essenti   it s necessari printer   there way i without printer   a printer essenti me i must one   i need printer print paper   it necessari print paper printer   the paper i need print need print printer   i want print document printer   use printer print paper necessari me   pleas send new email   i need new email address   i would like new email address   pleas give email address   would mind send email   send email mind   i would like email address   let know email address   i need work email   i d want get new email address   i access internet i email address   my work email necessari   i access email account   i requir new email address   a new email address requir   i m desper need fresh email address  i d like new email address   i d want new email address sent   if mind send email   if mind pleas send email   i m new employe email address   i m brandnew employe email address   i m new employe without email address   depart email   user email   email srevic   hi there   how you   is anyon there   hey   hola   hello   good day   hello there   how you   bye   see later   goodby   thank   thank you   that s help   awesom thank   thank help me   ahmad alharthi   faisal alsufyani   surayyi alqahtani   shoug alkhathran   nawaf almutairi   extens   badg   em   build   floor   room '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns= df.patterns.str.cat(sep=' ')\n",
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'printer',\n",
       " 'softwar',\n",
       " 'new',\n",
       " 'need',\n",
       " 'offic',\n",
       " 'email',\n",
       " 'want',\n",
       " 'the',\n",
       " 'my',\n",
       " 'problem',\n",
       " 'work',\n",
       " 'lnk',\n",
       " 'toner',\n",
       " 'black',\n",
       " 'would',\n",
       " 'pleas',\n",
       " 'one',\n",
       " 'there',\n",
       " 'm']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(patterns)\n",
    "\n",
    "vocabulary = set(tokens)\n",
    "print(len(vocabulary))\n",
    "\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings with multiword ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printer 95\n",
      "softwar 79\n",
      "new 56\n",
      "need 55\n",
      "offic 43\n",
      "email 26\n",
      "want 25\n",
      "problem 20\n",
      "work 20\n",
      "lnk 19\n",
      "toner 19\n",
      "black 17\n",
      "would 16\n",
      "pleas 16\n",
      "one 16\n",
      "applic 15\n",
      "address 15\n",
      "like 12\n",
      "cartridg 11\n",
      "instal 11\n",
      "open 10\n",
      "uninstal 10\n",
      "program 9\n",
      "get 9\n",
      "print 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>printer</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>softwar</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>offic</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>email</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>want</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>problem</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>work</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lnk</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>toner</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>black</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>would</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>pleas</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>one</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>applic</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>address</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>like</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cartridg</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>instal</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>open</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>uninstal</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>program</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>get</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>print</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  count\n",
       "0    printer     95\n",
       "1    softwar     79\n",
       "2        new     56\n",
       "3       need     55\n",
       "4      offic     43\n",
       "5      email     26\n",
       "6       want     25\n",
       "7    problem     20\n",
       "8       work     20\n",
       "9        lnk     19\n",
       "10     toner     19\n",
       "11     black     17\n",
       "12     would     16\n",
       "13     pleas     16\n",
       "14       one     16\n",
       "15    applic     15\n",
       "16   address     15\n",
       "17      like     12\n",
       "18  cartridg     11\n",
       "19    instal     11\n",
       "20      open     10\n",
       "21  uninstal     10\n",
       "22   program      9\n",
       "23       get      9\n",
       "24     print      8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def top_n_words(corpus, n=None):\n",
    "# هنا ابدا اسوي يونيقرام للباترنز باي كاونت \n",
    "    cv = CountVectorizer(stop_words= stopwords.words(\"english\")).fit(corpus) \n",
    "    bag_of_words = cv.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = top_n_words(df['patterns'], 25)\n",
    "for word, count in common_words:\n",
    "    \n",
    "    print(word, count)\n",
    "df_unigrams = pd.DataFrame(common_words,columns = ['word' , 'count'])\n",
    "df_unigrams.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-igrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new printer</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>printer offic</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>email address</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>softwar applic</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>need new</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>toner cartridg</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>problem softwar</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>softwar work</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>black lnk</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>get new</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  count\n",
       "0      new printer     33\n",
       "1    printer offic     25\n",
       "2    email address     15\n",
       "3   softwar applic     12\n",
       "4         need new     12\n",
       "5   toner cartridg     11\n",
       "6  problem softwar     11\n",
       "7     softwar work     11\n",
       "8        black lnk     10\n",
       "9          get new      8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_n_words(corpus, n=None):\n",
    "    cv = CountVectorizer(stop_words= stopwords.words(\"english\"),ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = cv.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = top_n_words(df['patterns'], 20)\n",
    "\n",
    "df_bigrams = pd.DataFrame(common_words, columns = ['word' , 'count'])\n",
    "df_bigrams.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new printer offic</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>need new printer</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new email address</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>want order new</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get new printer</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>there problem softwar</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the softwar work</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>toner cartridg printer</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>is possibl printer</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>possibl printer offic</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>printer need one</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>printer print paper</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>printer toner cartridg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>softwar work me</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>softwar give problem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the softwar use</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>offic need new</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pleas provid new</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>provid new printer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>like new printer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      word  count\n",
       "0        new printer offic     16\n",
       "1         need new printer      8\n",
       "2        new email address      7\n",
       "3           want order new      6\n",
       "4          get new printer      6\n",
       "5    there problem softwar      5\n",
       "6         the softwar work      5\n",
       "7   toner cartridg printer      4\n",
       "8       is possibl printer      4\n",
       "9    possibl printer offic      4\n",
       "10        printer need one      4\n",
       "11     printer print paper      3\n",
       "12  printer toner cartridg      3\n",
       "13         softwar work me      3\n",
       "14    softwar give problem      3\n",
       "15         the softwar use      3\n",
       "16          offic need new      3\n",
       "17        pleas provid new      3\n",
       "18      provid new printer      3\n",
       "19        like new printer      3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_n_words(corpus, n=None):\n",
    "    vect = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n",
    "    bag_of_words = vect.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = top_n_words(df['patterns'],20)\n",
    "#for word, count in common_words:\n",
    "    \n",
    "    #print(word, count)\n",
    "df_trigram = pd.DataFrame(common_words, columns = ['word' , 'count'])\n",
    "df_trigram.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' i d want order new toner cartridg']\n",
      "___________Convert sentences into words________________\n",
      "['i', 'd', 'want', 'order', 'new', 'toner', 'cartridg']\n"
     ]
    }
   ],
   "source": [
    "sentences = df.iloc[19] # how can i choose randomly sentences ?\n",
    "sentences['patterns']\n",
    "print (nltk.sent_tokenize(sentences['patterns']))\n",
    "print('___________Convert sentences into words________________')\n",
    "print (nltk.word_tokenize(sentences['patterns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bag of word \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "count_vector.fit(df['patterns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc_array = count_vector.transform(df['patterns'][:10]).toarray()\n",
    "doc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>address</th>\n",
       "      <th>ahmad</th>\n",
       "      <th>alharthi</th>\n",
       "      <th>alkhathran</th>\n",
       "      <th>all</th>\n",
       "      <th>almutairi</th>\n",
       "      <th>alqahtani</th>\n",
       "      <th>alsufyani</th>\n",
       "      <th>...</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>we</th>\n",
       "      <th>wish</th>\n",
       "      <th>without</th>\n",
       "      <th>work</th>\n",
       "      <th>workplac</th>\n",
       "      <th>would</th>\n",
       "      <th>yellow</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patterns</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>my printer capabl print document</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my printer print paper</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i think lnk empti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i believ lnk empti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the lnk appear empti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    access  account  address  ahmad  alharthi  \\\n",
       "patterns                                                                        \n",
       " my printer capabl print document        0        0        0      0         0   \n",
       " my printer print paper                  0        0        0      0         0   \n",
       " i think lnk empti                       0        0        0      0         0   \n",
       " i believ lnk empti                      0        0        0      0         0   \n",
       " the lnk appear empti                    0        0        0      0         0   \n",
       "\n",
       "                                    alkhathran  all  almutairi  alqahtani  \\\n",
       "patterns                                                                    \n",
       " my printer capabl print document            0    0          0          0   \n",
       " my printer print paper                      0    0          0          0   \n",
       " i think lnk empti                           0    0          0          0   \n",
       " i believ lnk empti                          0    0          0          0   \n",
       " the lnk appear empti                        0    0          0          0   \n",
       "\n",
       "                                    alsufyani  ...  want  way  we  wish  \\\n",
       "patterns                                       ...                        \n",
       " my printer capabl print document           0  ...     0    0   0     0   \n",
       " my printer print paper                     0  ...     0    0   0     0   \n",
       " i think lnk empti                          0  ...     0    0   0     0   \n",
       " i believ lnk empti                         0  ...     0    0   0     0   \n",
       " the lnk appear empti                       0  ...     0    0   0     0   \n",
       "\n",
       "                                    without  work  workplac  would  yellow  \\\n",
       "patterns                                                                     \n",
       " my printer capabl print document         0     0         0      0       0   \n",
       " my printer print paper                   0     0         0      0       0   \n",
       " i think lnk empti                        0     0         0      0       0   \n",
       " i believ lnk empti                       0     0         0      0       0   \n",
       " the lnk appear empti                     0     0         0      0       0   \n",
       "\n",
       "                                    you  \n",
       "patterns                                 \n",
       " my printer capabl print document     0  \n",
       " my printer print paper               0  \n",
       " i think lnk empti                    0  \n",
       " i believ lnk empti                   0  \n",
       " the lnk appear empti                 0  \n",
       "\n",
       "[5 rows x 166 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_matrix = pd.DataFrame(doc_array,index=df['patterns'][:10],columns=count_vector.get_feature_names())\n",
    "frequency_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "words = ['printer','software', 'toner', 'email']  \n",
    "\n",
    "vectors = np.array([[0.6,   0.8], \n",
    "                    [0.8, 0.6],\n",
    "                   [.2,.4],[.4,.2]]\n",
    "                 ) \n",
    "\n",
    "plt.plot(vectors[:,0], vectors[:,1], 'o')  \n",
    "plt.xlim(0, 1.5)  \n",
    "plt.ylim(0, 1.5)  \n",
    "for word, x, y in zip(words, vectors[:,0], vectors[:,1]):  \n",
    "    plt.annotate(word, (x, y), size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>address</th>\n",
       "      <th>ahmad</th>\n",
       "      <th>alharthi</th>\n",
       "      <th>alkhathran</th>\n",
       "      <th>all</th>\n",
       "      <th>almutairi</th>\n",
       "      <th>alqahtani</th>\n",
       "      <th>alsufyani</th>\n",
       "      <th>...</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>we</th>\n",
       "      <th>wish</th>\n",
       "      <th>without</th>\n",
       "      <th>work</th>\n",
       "      <th>workplac</th>\n",
       "      <th>would</th>\n",
       "      <th>yellow</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patterns</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>my printer capabl print document</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my printer print paper</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i think lnk empti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i believ lnk empti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the lnk appear empti</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 166 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    access  account  address  ahmad  alharthi  \\\n",
       "patterns                                                                        \n",
       " my printer capabl print document        0        0        0      0         0   \n",
       " my printer print paper                  0        0        0      0         0   \n",
       " i think lnk empti                       0        0        0      0         0   \n",
       " i believ lnk empti                      0        0        0      0         0   \n",
       " the lnk appear empti                    0        0        0      0         0   \n",
       "\n",
       "                                    alkhathran  all  almutairi  alqahtani  \\\n",
       "patterns                                                                    \n",
       " my printer capabl print document            0    0          0          0   \n",
       " my printer print paper                      0    0          0          0   \n",
       " i think lnk empti                           0    0          0          0   \n",
       " i believ lnk empti                          0    0          0          0   \n",
       " the lnk appear empti                        0    0          0          0   \n",
       "\n",
       "                                    alsufyani  ...  want  way  we  wish  \\\n",
       "patterns                                       ...                        \n",
       " my printer capabl print document           0  ...     0    0   0     0   \n",
       " my printer print paper                     0  ...     0    0   0     0   \n",
       " i think lnk empti                          0  ...     0    0   0     0   \n",
       " i believ lnk empti                         0  ...     0    0   0     0   \n",
       " the lnk appear empti                       0  ...     0    0   0     0   \n",
       "\n",
       "                                    without  work  workplac  would  yellow  \\\n",
       "patterns                                                                     \n",
       " my printer capabl print document         0     0         0      0       0   \n",
       " my printer print paper                   0     0         0      0       0   \n",
       " i think lnk empti                        0     0         0      0       0   \n",
       " i believ lnk empti                       0     0         0      0       0   \n",
       " the lnk appear empti                     0     0         0      0       0   \n",
       "\n",
       "                                    you  \n",
       "patterns                                 \n",
       " my printer capabl print document     0  \n",
       " my printer print paper               0  \n",
       " i think lnk empti                    0  \n",
       " i believ lnk empti                   0  \n",
       " the lnk appear empti                 0  \n",
       "\n",
       "[5 rows x 166 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_matrix = pd.DataFrame(doc_array,index=df['patterns'][:10],columns=count_vector.get_feature_names())\n",
    "frequency_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      my printer capabl print document \n",
      "1                my printer print paper \n",
      "2                     i think lnk empti \n",
      "3                    i believ lnk empti \n",
      "4                  the lnk appear empti \n",
      "                     ...                \n",
      "95                              it work \n",
      "96                   we problem softwar \n",
      "97                       softwar broken \n",
      "98                     i troubl softwar \n",
      "99                there problem softwar \n",
      "Name: patterns, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = (df)\n",
    "\n",
    "shelley_text = r.patterns\n",
    "\n",
    "print(shelley_text[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_only = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
    "                          \" \",          # Replace all non-letters with spaces\n",
    "                          str(df.patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = \"&lrm;Some Time W&zwnj;e\"\n",
    "mystring  = re.sub(r\"&lrm;\", \"\", mystring)\n",
    "mystring  = re.sub(r\"&zwnj;\", \"\", mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['patterns1']=df['patterns'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       my printer capabl print document  1                 my printer print paper  2                      i think lnk empti  3                     i believ lnk empti  4                   the lnk appear empti                        ...                 273                                 badg  274   \n"
     ]
    }
   ],
   "source": [
    "# Next, we'll pre-process the text by adding periods where\n",
    "# weant sentence breaks to occur.\n",
    "\n",
    "# replace 2 or more new lines with a period and one new line\n",
    "shelley_text = re.sub(r\"\\n{2,}\", \".  \", str(shelley_text))\n",
    "\n",
    "# replace single new lines with a space\n",
    "shelley_text = re.sub(r\"\\n\", \" \", str(shelley_text))\n",
    "print(shelley_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 0 sentences!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssq12\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Now we're going to use NLTK's sentence tokenizer to split the text into sentences. Be careful! The sentence tokenizer is sensitive to preprocessing steps like lowercasing text.\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = sent_tokenize(shelley_text)\n",
    "\n",
    "sentences = sentences[2:]\n",
    "\n",
    "print(f\"We found {len(sentences)} sentences!\", end=\"\\n\\n\")\n",
    "\n",
    "for sent in sentences[:2]:\n",
    "    print(sentences[:200], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector = CountVectorizer()\n",
    "count_vector.fit(df['patterns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next step uses CountVectorizer to build a sentence vocabulary\n",
    "\n",
    "countvect = CountVectorizer(max_features=2500)\n",
    "\n",
    "bow = countvect.fit_transform(df.patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "#Dimension of vectors we are generating\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "#Creating Word Vectors by Word2Vec Method (takes time...)\n",
    "w2v_model = gensim.models.Word2Vec(sentences=X,window=2, min_count=1)\n",
    "#vocab size\n",
    "len(w2v_model.wv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_length=11\n",
    "max_features=100 \n",
    "\n",
    "x_tokenizer=Tokenizer(max_features)\n",
    "y_tokenizer=Tokenizer(max_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 222\n",
      "Test Data: 56\n"
     ]
    }
   ],
   "source": [
    "X=df['patterns'].values\n",
    "y=df['Class'].values\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "print('Training Data:', X_train.shape[0])\n",
    "print('Test Data:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100\n",
    "max_text_length=11\n",
    "x_tokenizer=Tokenizer(max_features)\n",
    "\n",
    "y_tokenizer=Tokenizer(max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "x_tokenizer.fit_on_texts(X)\n",
    "word_index = x_tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "max_features=100\n",
    "max_text_length=11\n",
    "\n",
    "\n",
    "x_tokenizer=Tokenizer(max_features)\n",
    "y_tokenizer=Tokenizer(max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most common words are: []\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 most common words are:', collections.Counter(x_tokenizer.word_counts).most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training a Simple RNN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_len = 11 # standardized length of each word sequence \n",
    "max_vocab = 15000 # max number of words to consider when tokenizing (based on freq)\n",
    "\n",
    "# fit tokenizer vocab (note that it lowercases and strips punct)\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(df.patterns)\n",
    "\n",
    "# standard train/val split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.patterns, df.Class, \n",
    "                                                    test_size=0.2, random_state = 42)\n",
    "\n",
    "# multiclass output formatting\n",
    "y_train_cat = np_utils.to_categorical(y_train)\n",
    "\n",
    "# convert train and val texts to token sequences of standardized length 50,\n",
    "# padding fills leading 0s in or cuts off sequence at 50th word\n",
    "X_train = tokenizer.texts_to_sequences(X_train) \n",
    "X_train = pad_sequences(X_train, maxlen=seq_len)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 11)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 11, 40)            600000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               18688     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 19)                627       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 621,395\n",
      "Trainable params: 621,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embedding_dim = 40 # hyper-parameter \n",
    "\n",
    "inp = Input(shape=(seq_len,)) # must specify format of input layer\n",
    "x = Embedding(max_vocab, embedding_dim)(inp) # model learns its own word embeddings\n",
    "x = Bidirectional(LSTM(32, recurrent_dropout=.3))(x) # bi-LSTM with regularization\n",
    "x = Dense(32)(x)\n",
    "y = Dense(19, activation='softmax')(x)\n",
    "\n",
    "NN = Model(inp, y)\n",
    "NN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 2.9509 - accuracy: 0.0113 - val_loss: 2.9408 - val_accuracy: 0.0667\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.9399 - accuracy: 0.0734 - val_loss: 2.9316 - val_accuracy: 0.0889\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 2.9290 - accuracy: 0.1186 - val_loss: 2.9223 - val_accuracy: 0.1333\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 2.9180 - accuracy: 0.1356 - val_loss: 2.9130 - val_accuracy: 0.1556\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 2.9069 - accuracy: 0.1525 - val_loss: 2.9033 - val_accuracy: 0.1778\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 2.8953 - accuracy: 0.1921 - val_loss: 2.8932 - val_accuracy: 0.2000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 2.8832 - accuracy: 0.2542 - val_loss: 2.8826 - val_accuracy: 0.2667\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.8706 - accuracy: 0.2881 - val_loss: 2.8713 - val_accuracy: 0.3778\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.8567 - accuracy: 0.4181 - val_loss: 2.8591 - val_accuracy: 0.4667\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2.8419 - accuracy: 0.4576 - val_loss: 2.8461 - val_accuracy: 0.4889\n"
     ]
    }
   ],
   "source": [
    "NN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = NN.fit(X_train, y_train_cat, \n",
    "                 validation_split=.2,\n",
    "                 epochs=10, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training an RNN with Transferred Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "# change the path to point to your pretrained google vectors file\n",
    "w2v_file = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "# load the w2v vectors using gensim\n",
    "word_vectors = KeyedVectors.load_word2vec_format(w2v_file, binary=True)\n",
    "\n",
    "embedding_dim = 300 # w2v embedding dim\n",
    "\n",
    "# use the gensim model to build a numpy array of embeddings,\n",
    "# we'll feed this array to the keras embeddings layer.\n",
    "# each row i of the array will correspond to the word token assigned to that value \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except: # word in our data vocab is missing in w2v, will use 0 vector for that word\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 11)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 11, 300)           52500     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 512)              1140736   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 19)                2451      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,261,351\n",
      "Trainable params: 1,208,851\n",
      "Non-trainable params: 52,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "inp = Input(shape=(seq_len,))\n",
    "x = Embedding(len(word_index) + 1,\n",
    "              embedding_dim,\n",
    "              weights=[embedding_matrix], # where we feed the pretrained vecs\n",
    "              trainable=False)(inp) # freeze these parameters in the model\n",
    "\n",
    "x = Bidirectional(LSTM(256, recurrent_dropout = .3))(x)\n",
    "x = Dense(128, activation='relu')(x) # fully connected layer on top of the output of the bi-LSTM\n",
    "x = Dropout(.3)(x)\n",
    "y = Dense(19, activation='softmax')(x)\n",
    "\n",
    "NN = Model(inp, y)\n",
    "NN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 12s 12s/step - loss: 0.3562 - accuracy: 0.9096 - val_loss: 0.7263 - val_accuracy: 0.7556\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 0.3841 - accuracy: 0.8531 - val_loss: 0.6434 - val_accuracy: 0.8222\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 406ms/step - loss: 0.3519 - accuracy: 0.8757 - val_loss: 0.6085 - val_accuracy: 0.8222\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 0.2840 - accuracy: 0.9209 - val_loss: 0.6367 - val_accuracy: 0.8444\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 0.2620 - accuracy: 0.9153 - val_loss: 0.7091 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a5c1fb4ac0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "NN.fit(X_train, y_train_cat, \n",
    "       validation_split=.2,\n",
    "       epochs=5, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best accuracy: 0.9209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_RNN_with_Transferred_Word_Embeddings.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extension: Training an RNN with Both Custom and Transferred Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 11, 300)      52500       ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 11, 40)       600000      ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 11, 340)      0           ['embedding_2[0][0]',            \n",
      "                                                                  'embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 512)         1222656     ['concatenate[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          65664       ['bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 19)           2451        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,943,271\n",
      "Trainable params: 1,890,771\n",
      "Non-trainable params: 52,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_dim_pre = 300\n",
    "embedding_dim_custom = 40\n",
    "\n",
    "inp = Input(shape=(seq_len,))\n",
    "emb_pre = Embedding(len(word_index) + 1,\n",
    "                    embedding_dim_pre,\n",
    "                    weights=[embedding_matrix], # where we feed the pretrained vecs\n",
    "                    trainable=False)(inp) # freeze these parameters in the model\n",
    "\n",
    "emb_cus = Embedding(max_vocab, embedding_dim_custom)(inp) # customized embedding \n",
    "\n",
    "emb = Concatenate()([emb_pre, emb_cus]) # concatenate embeddings before recurrent layers\n",
    "\n",
    "x = Bidirectional(LSTM(256, recurrent_dropout = .3))(emb)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.3)(x)\n",
    "y = Dense(19, activation='softmax')(x)\n",
    "\n",
    "NN = Model(inp, y)\n",
    "NN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 2.9421 - accuracy: 0.0791 - val_loss: 2.8679 - val_accuracy: 0.4222\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 387ms/step - loss: 2.8627 - accuracy: 0.3672 - val_loss: 2.7943 - val_accuracy: 0.4889\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 2.7891 - accuracy: 0.4633 - val_loss: 2.7002 - val_accuracy: 0.4889\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 2.6828 - accuracy: 0.5198 - val_loss: 2.5780 - val_accuracy: 0.4889\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 2.5669 - accuracy: 0.4915 - val_loss: 2.4216 - val_accuracy: 0.4889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a5969219a0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "NN.fit(X_train, y_train_cat, \n",
    "       validation_split=.2,\n",
    "       epochs=5, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Twilio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  is a modern communication API Used by developers for establishing communications.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Google Cloud Platform \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is a part of Google Cloud, which is a public cloud infrastructure and application programming interfaces (APIs) for machine learning and enterprise mapping services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ssq12\\\\OneDrive\\\\سطح المكتب\\\\printer'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
